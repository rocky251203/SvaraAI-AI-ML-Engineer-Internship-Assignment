# -*- coding: utf-8 -*-
"""SvaraAI – AI/ML Engineer Internship Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QKnFpoxswqskJKAN6OxrsRI1-TgIhZs9
"""

!pip install -q transformers datasets evaluate scikit-learn lightgbm joblib

from google.colab import files
import pandas as pd

uploaded = files.upload()
df = pd.read_csv(next(iter(uploaded.keys())))

df.head(10)
df.shape
df.info()
# Check class distribution
print(df['label'].value_counts())
print(df.isnull().sum())
print("duplicates:", df.duplicated(subset=['reply']).sum())

import re
import numpy as np
import pandas as pd

TEXT_COL = 'reply'
LABEL_COL = 'label'

def clean_text(s):
    if pd.isna(s): return ""
    s = str(s).lower()
    s = re.sub(r'http\S+|www\.\S+', ' ', s)
    s = re.sub(r'\S+@\S+', ' ', s)
    s = re.sub(r'\d+', ' ', s)
    s = re.sub(r'[^a-z\s]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

df['text_clean'] = df[TEXT_COL].apply(clean_text)
df[LABEL_COL] = df[LABEL_COL].astype(str).str.lower().str.strip()
df = df[df['text_clean'].str.len() > 0]  # drop empty

from sklearn.model_selection import train_test_split
labels = sorted(df[LABEL_COL].unique())
label2id = {l:i for i,l in enumerate(labels)}
id2label = {i:l for l,i in label2id.items()}
df['label_id'] = df[LABEL_COL].map(label2id)

# Stratified split: train (80%), test (20%) then split train->train+val if desired
X = df['text_clean'].values
y = df['label_id'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.1, stratify=y_train, random_state=42
)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import joblib

tf = TfidfVectorizer(max_features=10000, ngram_range=(1,2), min_df=2)
Xtr_tfidf = tf.fit_transform(X_train)
Xval_tfidf = tf.transform(X_val)
Xtest_tfidf = tf.transform(X_test)

lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)
lr.fit(Xtr_tfidf, y_train)

for name, X_, y_ in [('val', Xval_tfidf, y_val), ('test', Xtest_tfidf, y_test)]:
    preds = lr.predict(X_)
    print(f"=== {name} ===")
    print("Accuracy:", accuracy_score(y_, preds))
    print("Macro F1:", f1_score(y_, preds, average='macro'))
    print(classification_report(y_, preds, target_names=[id2label[i] for i in sorted(id2label)]))

joblib.dump(tf, 'tfidf_vectorizer.joblib')
joblib.dump(lr, 'logistic_regression_tfidf.joblib')

pip install -U lightgbm

!pip install -U lightgbm
import lightgbm as lgb
print(lgb.__version__)

import lightgbm as lgb
from lightgbm import early_stopping, log_evaluation
from sklearn.metrics import accuracy_score, f1_score

# Create datasets
dtrain = lgb.Dataset(Xtr_tfidf, label=y_train)
dval = lgb.Dataset(Xval_tfidf, label=y_val, reference=dtrain)

# Parameters
params = {
    'objective': 'multiclass',
    'num_class': len(labels),
    'metric': 'multi_logloss',
    'verbosity': -1,
    'learning_rate': 0.05
}

# Train with callbacks instead of early_stopping_rounds
bst = lgb.train(
    params,
    dtrain,
    valid_sets=[dval],
    num_boost_round=1000,
    callbacks=[
        early_stopping(stopping_rounds=30),
        log_evaluation(period=50)  # optional, logs every 50 rounds
    ]
)

# Predictions
y_pred_prob = bst.predict(Xtest_tfidf)
y_pred = y_pred_prob.argmax(axis=1)

# Metrics
print("LGB test acc:", accuracy_score(y_test, y_pred))
print("LGB test macro F1:", f1_score(y_test, y_pred, average='macro'))

bst.save_model('lightgbm_tfidf.txt')

!pip install evaluate

import evaluate

# Load metric
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric.compute(predictions=preds, references=labels)

!pip install --upgrade transformers

evaluation_strategy='epoch'
save_strategy='epoch'
load_best_model_at_end=True
metric_for_best_model='f1_macro'

#pip install --upgrade transformers datasets evaluate

import pandas as pd
import numpy as np
import torch
from datasets import Dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)

# epare model and tokenizer
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare Hugging Face datasets
train_df = pd.DataFrame({'text': X_train, 'label': y_train})
val_df   = pd.DataFrame({'text': X_val,   'label': y_val})
test_df  = pd.DataFrame({'text': X_test,  'label': y_test})

train_ds = Dataset.from_pandas(train_df)
val_ds   = Dataset.from_pandas(val_df)
test_ds  = Dataset.from_pandas(test_df)

# Tokenization function
def tokenize_fn(example):
    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)

# Apply tokenization
train_ds = train_ds.map(tokenize_fn, batched=True)
val_ds   = val_ds.map(tokenize_fn, batched=True)
test_ds  = test_ds.map(tokenize_fn, batched=True)

# Set format for PyTorch
columns = ['input_ids', 'attention_mask', 'label']
train_ds.set_format(type='torch', columns=columns)
val_ds.set_format(type='torch', columns=columns)
test_ds.set_format(type='torch', columns=columns)

# Load model
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))
# Metrics using evaluate
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_metric.compute(predictions=preds, references=labels)['accuracy']
    f1 = f1_metric.compute(predictions=preds, references=labels, average='macro')['f1']
    return {'accuracy': acc, 'f1_macro': f1}
#  Training arguments
training_args = TrainingArguments(
    output_dir='./distilbert_finetuned',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    num_train_epochs=3,
    logging_steps=50,
    save_total_limit=2,
    do_train=True,
    do_eval=True
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

trainer.save_model('./distilbert_finetuned')
tokenizer.save_pretrained('./distilbert_finetuned')

test_metrics = trainer.evaluate(test_ds)
print("Test set metrics:", test_metrics)

# Baseline LR on test
lr_preds = lr.predict(Xtest_tfidf)
from sklearn.metrics import classification_report
print("LR report:")
print(classification_report(y_test, lr_preds, target_names=[id2label[i] for i in sorted(id2label)]))

# Transformer test predictions using Trainer.predict
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np

model_tf = AutoModelForSequenceClassification.from_pretrained('./distilbert_finetuned')
tokenizer = AutoTokenizer.from_pretrained('./distilbert_finetuned')

def predict_transformer(texts):
    enc = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')
    if torch.cuda.is_available():
        model_tf.cuda()
        enc = {k:v.cuda() for k,v in enc.items()}
    with torch.no_grad():
        out = model_tf(**enc)
        probs = torch.nn.functional.softmax(out.logits, dim=-1).cpu().numpy()
    preds = probs.argmax(axis=1)
    return preds, probs

preds_tf, probs_tf = predict_transformer(list(X_test))
print("Transformer report:")
print(classification_report(y_test, preds_tf, target_names=[id2label[i] for i in sorted(id2label)]))

from google.colab import drive
drive.mount('/content/my_drive')

import os
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from google.colab import drive

# 1️⃣ Mount Google Drive
drive.mount('/content/drive')

# 2️⃣ Define paths
save_folder = '/content/drive/MyDrive/svaraai/'
tfidf_path = os.path.join(save_folder, 'tfidf_vectorizer.joblib')
lr_path    = os.path.join(save_folder, 'logistic_regression_tfidf.joblib')
distilbert_src = './distilbert_finetuned'
distilbert_dst = os.path.join(save_folder, 'distilbert_finetuned')

# 3️⃣ Create folder if it doesn't exist
os.makedirs(save_folder, exist_ok=True)

# 4️⃣ Example: create TF-IDF + Logistic Regression (replace with your trained models)
tf = TfidfVectorizer()
lr = LogisticRegression()

# If you have training data, fit them first:
# tfidf_matrix = tf.fit_transform(X_train)
# lr.fit(tfidf_matrix, y_train)

# 5️⃣ Save the TF-IDF vectorizer and Logistic Regression
joblib.dump(tf, tfidf_path)
joblib.dump(lr, lr_path)

# 6️⃣ Copy the distilbert_finetuned folder to Google Drive
!cp -r "{distilbert_src}" "{distilbert_dst}"

print("✅ Models and transformer saved successfully!")

import random, numpy as np, torch
random.seed(42); np.random.seed(42); torch.manual_seed(42)
if torch.cuda.is_available(): torch.cuda.manual_seed_all(42)

