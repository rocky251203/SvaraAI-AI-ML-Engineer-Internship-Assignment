{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OByRVBKv6-q4",
        "outputId": "62a3c27e-4794-4352-88ef-bf41d2a833ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No model artifacts found. Please place distilbert_finetuned/ or tfidf_vectorizer.joblib + classifier in the app folder.\n"
          ]
        }
      ],
      "source": [
        "# app.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pathlib import Path\n",
        "import json, joblib, numpy as np\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Transformer imports (optional; only needed when transformer is present)\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    import torch\n",
        "except Exception:\n",
        "    AutoTokenizer = None\n",
        "    AutoModelForSequenceClassification = None\n",
        "    torch = None\n",
        "\n",
        "app = FastAPI(title=\"Reply Classification API\", version=\"1.0\")\n",
        "\n",
        "# Request schema\n",
        "class PredictRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# Basic cleaning function (same as training)\n",
        "def clean_text(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r'http\\S+|www\\.\\S+', ' ', s)\n",
        "    s = re.sub(r'\\S+@\\S+', ' ', s)\n",
        "    s = re.sub(r'\\d+', ' ', s)\n",
        "    s = re.sub(r'[^a-z\\s]', ' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "# Load metadata (label mapping)\n",
        "BASE = Path(\".\")   # current working directory\n",
        "metadata_path = BASE / \"metadata.json\"\n",
        "if metadata_path.exists():\n",
        "    with open(metadata_path, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "    label_mapping = metadata.get(\"label_mapping\", None)\n",
        "    # label_mapping: { \"positive\": 2, ... } -> convert to inv map\n",
        "    inv_label_mapping = {int(v): k for k, v in label_mapping.items()} if label_mapping else None\n",
        "else:\n",
        "    inv_label_mapping = None\n",
        "\n",
        "# Try to load transformer\n",
        "transformer_model = None\n",
        "tokenizer = None\n",
        "use_transformer = False\n",
        "tfidf = None\n",
        "clf = None\n",
        "lgb_booster = None\n",
        "\n",
        "if (BASE / \"distilbert_finetuned\").exists() and AutoTokenizer is not None:\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(str(BASE / \"distilbert_finetuned\"))\n",
        "        transformer_model = AutoModelForSequenceClassification.from_pretrained(str(BASE / \"distilbert_finetuned\"))\n",
        "        # If GPU available, move model to GPU\n",
        "        if torch is not None and torch.cuda.is_available():\n",
        "            transformer_model.cuda()\n",
        "        use_transformer = True\n",
        "        print(\"Using transformer model: distilbert_finetuned\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load transformer:\", e)\n",
        "        use_transformer = False\n",
        "\n",
        "# Fallback: load TF-IDF + classifier (joblib)\n",
        "if not use_transformer:\n",
        "    if (BASE / \"tfidf_vectorizer.joblib\").exists() and (BASE / \"logistic_regression_tfidf.joblib\").exists():\n",
        "        tfidf = joblib.load(str(BASE / \"tfidf_vectorizer.joblib\"))\n",
        "        clf = joblib.load(str(BASE / \"logistic_regression_tfidf.joblib\"))\n",
        "        print(\"Using TF-IDF + classifier model\")\n",
        "    elif (BASE / \"tfidf_vectorizer.joblib\").exists() and (BASE / \"lightgbm_tfidf.txt\").exists():\n",
        "        # Load TF-IDF and LightGBM booster\n",
        "        tfidf = joblib.load(str(BASE / \"tfidf_vectorizer.joblib\"))\n",
        "        try:\n",
        "            import lightgbm as lgb\n",
        "            lgb_booster = lgb.Booster(model_file=str(BASE / \"lightgbm_tfidf.txt\"))\n",
        "            print(\"Using TF-IDF + LightGBM model\")\n",
        "        except Exception as e:\n",
        "            print(\"LightGBM not available:\", e)\n",
        "    else:\n",
        "        print(\"No model artifacts found. Please place distilbert_finetuned/ or tfidf_vectorizer.joblib + classifier in the app folder.\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\", \"model\": (\"distilbert\" if use_transformer else (\"tfidf_lr\" if clf is not None else \"none\"))}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(req: PredictRequest):\n",
        "    text = req.text\n",
        "    if not text or not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        raise HTTPException(status_code=400, detail=\"`text` is required and must be non-empty string.\")\n",
        "    # optional: limit text length\n",
        "    if len(text) > 5000:\n",
        "        text = text[:5000]\n",
        "\n",
        "    cleaned = clean_text(text)\n",
        "\n",
        "    # Transformer path\n",
        "    if use_transformer and tokenizer is not None and transformer_model is not None:\n",
        "        # tokenize\n",
        "        inputs = tokenizer(cleaned, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "        if torch is not None and torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "            transformer_model.cuda()\n",
        "        transformer_model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = transformer_model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "        pred_id = int(np.argmax(probs))\n",
        "        label = inv_label_mapping.get(pred_id, str(pred_id)) if inv_label_mapping else str(pred_id)\n",
        "        confidence = float(np.max(probs))\n",
        "        return {\"label\": label, \"confidence\": round(confidence, 4)}\n",
        "\n",
        "    # TF-IDF + sklearn classifier\n",
        "    if tfidf is not None and clf is not None:\n",
        "        X = tfidf.transform([cleaned])\n",
        "        if hasattr(clf, \"predict_proba\"):\n",
        "            probs = clf.predict_proba(X)[0]\n",
        "        else:\n",
        "            # fallback: attempt decision_function -> softmax\n",
        "            try:\n",
        "                scores = clf.decision_function(X)[0]\n",
        "                exp = np.exp(scores - np.max(scores))\n",
        "                probs = exp / exp.sum()\n",
        "            except Exception:\n",
        "                probs = None\n",
        "        pred_id = int(np.argmax(probs)) if probs is not None else int(clf.predict(X)[0])\n",
        "        label = inv_label_mapping.get(pred_id, str(pred_id)) if inv_label_mapping else str(pred_id)\n",
        "        confidence = float(np.max(probs)) if probs is not None else 1.0\n",
        "        return {\"label\": label, \"confidence\": round(confidence, 4)}\n",
        "\n",
        "    # TF-IDF + LightGBM booster\n",
        "    if tfidf is not None and lgb_booster is not None:\n",
        "        X = tfidf.transform([cleaned])\n",
        "        probs = lgb_booster.predict(X)[0]\n",
        "        pred_id = int(np.argmax(probs))\n",
        "        label = inv_label_mapping.get(pred_id, str(pred_id)) if inv_label_mapping else str(pred_id)\n",
        "        confidence = float(np.max(probs))\n",
        "        return {\"label\": label, \"confidence\": round(confidence, 4)}\n",
        "\n",
        "    raise HTTPException(status_code=500, detail=\"No model loaded on server.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(\"/content/app\", exist_ok=True)\n",
        "\n",
        "labels = {\"0\": \"negative\", \"1\": \"neutral\", \"2\": \"positive\"}\n",
        "\n",
        "# Now write the JSON file\n",
        "with open(\"/content/app/metadata.json\", \"w\") as f:\n",
        "    json.dump(labels, f)\n"
      ],
      "metadata": {
        "id": "wxBpE5clq8ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/app/metadata.json\", \"r\") as f:\n",
        "    id2label = json.load(f)"
      ],
      "metadata": {
        "id": "QD4k5qm8uX3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_path = \"./distilbert_finetuned\"  # make sure this folder exists\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "qItdt5QedRkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load model + tokenizer\n",
        "MODEL_PATH = \"distilbert_finetuned\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "# Load label metadata\n",
        "with open(\"metadata.json\", \"r\") as f:\n",
        "    id2label = json.load(f)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class InputText(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(input: InputText):\n",
        "    inputs = tokenizer(input.text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    conf, pred_id = torch.max(probs, dim=1)\n",
        "    return {\n",
        "        \"label\": id2label[str(pred_id.item())],\n",
        "        \"confidence\": float(conf.item())\n",
        "    }"
      ],
      "metadata": {
        "id": "q3s8eMFldUj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "tb6QfKPUqlr-"
      }
    }
  ]
}